# -*- coding: utf-8 -*-
"""CSE425_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IXAB1jg-EQCPlUCxtfj-lp8H2JB_rhxZ

# Importing Libraries
"""

import torch
import torch . nn as nn
import torch . optim as optim
from torch . utils . data import DataLoader
import torchvision . transforms as transforms
import torchvision . datasets as datasets

"""# Load Dataset and Create Dataloaders"""

import torch
from torchvision import datasets, transforms

transform = transforms . Compose ([
    transforms . ToTensor () ,
    transforms . Normalize ((0.5 ,) , (0.5 ,) )
    ])
train_dataset = datasets.MNIST ( root ='./ data ', train = True , transform =
  transform , download = True )
test_dataset = datasets.MNIST ( root ='./ data ', train = False , transform =
  transform )

train_loader = DataLoader ( train_dataset , batch_size = batch_size , shuffle =
  True )
test_loader = DataLoader ( test_dataset , batch_size = batch_size , shuffle =
  False )

"""===================================="""

!pip install scikit-learn

!pip install minisom

"""# **CNN-LSTM MODEL FOR FEATURE EXTRACTION**"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from minisom import MiniSom
import numpy as np
from tqdm import tqdm
from sklearn.metrics import normalized_mutual_info_score as nmi_score
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# CNN-LSTM Model
class CNNLSTM(nn.Module):
    def __init__(self):
        super(CNNLSTM, self).__init__()
        # CNN for spatial features
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # [B, 32, 28, 28]
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),                  # [B, 32, 14, 14]
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # [B, 64, 14, 14]
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),                  # [B, 64, 7, 7]
        )
        # LSTM for sequential features (treat image rows as sequences)
        self.lstm = nn.LSTM(
            input_size=64 * 7,  # Each row has 7 pixels (after pooling)
            hidden_size=128,
            num_layers=1,
            batch_first=True
        )


        # Feature projector
        self.fc = nn.Linear(128, 64)  # Final feature dimension
        # New decoder
        self.decoder = nn.Sequential(
            nn.Linear(64, 128),
            nn.Linear(128, 64*7*7),
            nn.Unflatten(1, (64, 7, 7)),
            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # CNN
        x = self.cnn(x)  # [B, 64, 7, 7]
        B, C, H, W = x.shape
        # Reshape for LSTM: [B, H, C*W] (treat each row as a sequence)
        x = x.permute(0, 2, 1, 3).reshape(B, H, C * W)  # [B, 7, 64*7]
        # LSTM
        _, (h_n, _) = self.lstm(x)  # h_n: [1, B, 128]
        features = h_n.squeeze(0)    # [B, 128]
        # Project to lower dimension
        features = self.fc(features) # [B, 64]
        return features

def count_parameters(model):
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total:,}")
    print(f"Trainable parameters: {trainable:,}")
    return total, trainable

# Instantiate and count
model = CNNLSTM().to(device)
count_parameters(model)

"""# SOM Clustering with 10 epochs"""

def train_and_cluster():
    # Load MNIST
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)

    # Initialize model and optimizer
    model = CNNLSTM().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Step 1: Train CNN-LSTM (unsupervised)
    print("Training CNN-LSTM...")
    model.train()
    for epoch in range(10):
        for batch, _ in tqdm(train_loader, desc=f"Epoch {epoch+1}/10"):
            batch = batch.to(device)
            optimizer.zero_grad()
            features = model(batch)
            loss = torch.mean(features.pow(2)) * 0.01   # Add a decoder network


            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()


    # Step 2: Extract features
    print("Extracting features...")
    model.eval()
    features = []
    true_labels = []
    with torch.no_grad():
        for batch, labels in tqdm(train_loader):
            batch = batch.to(device)
            feats = model(batch)
            features.append(feats.cpu().numpy())
            true_labels.append(labels.numpy())
    features = np.concatenate(features)
    true_labels = np.concatenate(true_labels)

    # Step 3: Train SOM
    print("Clustering with SOM...")
    som = MiniSom(
        x=30, y=30,          # 10x10 grid (adjust based on expected clusters)
        input_len=64,        # Must match CNN-LSTM output dimension (64)
        sigma=1.5,           # Neighborhood radius
        learning_rate=0.2,
        neighborhood_function='gaussian',
        random_seed=42
    )
    som.train_random(features, 10000)  # 1000 iterations

    # Assign pseudo-labels based on SOM winner neurons
    winner_coords = np.array([som.winner(x) for x in features])
    pseudo_labels = winner_coords[:, 0] * 10 + winner_coords[:, 1]  # Flatten grid to 1D labels

    # Step 4: Evaluate clustering (NMI)
    nmi = nmi_score(true_labels, pseudo_labels)
    print(f"Clustering NMI: {nmi:.4f}")

    # Step 5: Visualize SOM
    plt.figure(figsize=(10, 10))
    plt.pcolor(som.distance_map().T, cmap='bone_r')  # U-matrix
    plt.colorbar()
    for i, (x, y) in enumerate([som.winner(f) for f in features[:1000]]):  # Plot 1000 samples
        plt.text(x + 0.5, y + 0.5, str(true_labels[i]),
                color=plt.cm.tab10(true_labels[i] / 10),
                fontdict={'weight': 'bold', 'size': 8})
    plt.title("SOM Clustering (MNIST Digits Overlay)")
    plt.show()

if __name__ == "__main__":
    train_and_cluster()

"""# SOM Clustering with 15 epochs"""

def train_and_cluster():
    # Load MNIST
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)

    # Initialize model and optimizer
    model = CNNLSTM().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Step 1: Train CNN-LSTM (unsupervised)
    print("Training CNN-LSTM...")
    model.train()
    for epoch in range(15):
        for batch, _ in tqdm(train_loader, desc=f"Epoch {epoch+1}/10"):
            batch = batch.to(device)
            optimizer.zero_grad()
            features = model(batch)
            reconstruction = model.decoder(features)
            loss = F.mse_loss(reconstruction, batch) + 0.01*features.pow(2).mean()  # Need to add decoder



            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()


    # Step 2: Extract features
    print("Extracting features...")
    model.eval()
    features = []
    true_labels = []
    with torch.no_grad():
        for batch, labels in tqdm(train_loader):
            batch = batch.to(device)
            feats = model(batch)
            features.append(feats.cpu().numpy())
            true_labels.append(labels.numpy())
    features = np.concatenate(features)
    true_labels = np.concatenate(true_labels)

    # Step 3: Train SOM
    print("Clustering with SOM...")
    som = MiniSom(
        x=30, y=30,          # 10x10 grid (adjust based on expected clusters)
        input_len=64,        # Must match CNN-LSTM output dimension (64)
        sigma=1.5,           # Neighborhood radius
        learning_rate=0.2,
        neighborhood_function='gaussian',
        random_seed=42
    )
    som.train_random(features, 10000)  # 1000 iterations

    # Assign pseudo-labels based on SOM winner neurons
    winner_coords = np.array([som.winner(x) for x in features])
    pseudo_labels = winner_coords[:, 0] * 10 + winner_coords[:, 1]  # Flatten grid to 1D labels

    # Step 4: Evaluate clustering (NMI)
    nmi = nmi_score(true_labels, pseudo_labels)
    print(f"Clustering NMI: {nmi:.4f}")

    # Step 5: Visualize SOM
    plt.figure(figsize=(10, 10))
    plt.pcolor(som.distance_map().T, cmap='bone_r')  # U-matrix
    plt.colorbar()
    for i, (x, y) in enumerate([som.winner(f) for f in features[:1000]]):  # Plot 1000 samples
        plt.text(x + 0.5, y + 0.5, str(true_labels[i]),
                color=plt.cm.tab10(true_labels[i] / 10),
                fontdict={'weight': 'bold', 'size': 8})
    plt.title("SOM Clustering (MNIST Digits Overlay)")
    plt.show()

if __name__ == "__main__":
    train_and_cluster()

"""# K-means"""

def train_and_cluster():
    # Load MNIST
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)

    # Initialize model and optimizer
    model = CNNLSTM().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Step 1: Train CNN-LSTM (unsupervised)
    print("Training CNN-LSTM...")
    model.train()
    for epoch in range(15):
        for batch, _ in tqdm(train_loader, desc=f"Epoch {epoch+1}/10"):
            batch = batch.to(device)
            optimizer.zero_grad()
            features = model(batch)
            reconstruction = model.decoder(features)
            loss = F.mse_loss(reconstruction, batch) + 0.01*features.pow(2).mean()  # Need to add decoder



            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()


    # Step 2: Extract features
    print("Extracting features...")
    model.eval()
    features = []
    true_labels = []
    with torch.no_grad():
        for batch, labels in tqdm(train_loader):
            batch = batch.to(device)
            feats = model(batch)
            features.append(feats.cpu().numpy())
            true_labels.append(labels.numpy())
    features = np.concatenate(features)
    true_labels = np.concatenate(true_labels)

    # Step 3: Apply K-means clustering (instead of SOM)
    print("Clustering with K-means...")
    kmeans = KMeans(n_clusters=10, n_init=20, random_state=42)
    kmeans_labels = kmeans.fit_predict(features)

    # Step 4: Evaluate clustering (NMI)
    kmeans_nmi = nmi_score(true_labels, kmeans_labels)
    print(f"K-means NMI: {kmeans_nmi:.4f}")

    # Step 5: Visualize results
    plt.figure(figsize=(12, 5))

    # PCA for visualization
    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    features_2d = pca.fit_transform(features)

    # K-means visualization
    plt.subplot(121)
    plt.scatter(features_2d[:, 0], features_2d[:, 1], c=kmeans_labels, cmap='tab10', s=1)
    plt.title(f'K-means Clustering (NMI={kmeans_nmi:.3f})')

    # True labels visualization
    plt.subplot(122)
    plt.scatter(features_2d[:, 0], features_2d[:, 1], c=true_labels, cmap='tab10', s=1)
    plt.title('True Labels')

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    train_and_cluster()

"""# COMPARISON"""

import matplotlib.pyplot as plt
import numpy as np

# Your obtained NMI scores
kmeans_nmi = 0.1935
som_nmi = 0.0976

# Data for plotting
methods = ['K-means', 'SOM']
nmi_scores = [kmeans_nmi, som_nmi]
colors = ['#1f77b4', '#ff7f0e']  # Blue and orange

# Create figure
plt.figure(figsize=(8, 6), dpi=100)

# Create bars
bars = plt.bar(methods, nmi_scores, color=colors, width=0.6)

# Add value labels on top of each bar
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.4f}',
             ha='center', va='bottom',
             fontsize=12, fontweight='bold')

# Add horizontal line at random baseline (0) and perfect score (1)
plt.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.7)
plt.axhline(y=1, color='green', linestyle=':', linewidth=1, alpha=0.5)

# Chart formatting
plt.title('Clustering Performance Comparison\n(Normalized Mutual Information)',
          fontsize=14, pad=20)
plt.ylabel('NMI Score', fontsize=12)
plt.ylim(0, 0.3)  # Adjusted to show your values clearly
plt.grid(axis='y', linestyle='--', alpha=0.4)

# Annotate the better performing method
plt.annotate('Better clustering',
             xy=(0, kmeans_nmi), xytext=(0.2, 0.25),
             arrowprops=dict(facecolor='black', shrink=0.05),
             fontsize=12)

plt.tight_layout()
plt.show()

"""# Clustering Algorithm"""

# Commented out IPython magic to ensure Python compatibility.
# =============================================
# Clustering MNIST with PCA + K-Means (Option 1)
# =============================================

# Import additional required libraries
import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, adjusted_rand_score
import matplotlib.pyplot as plt
# %matplotlib inline

# 1. Load and Prepare MNIST Data
# ------------------------------
# (Reusing your existing transform and dataset)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Load MNIST (using same data as your original notebook)
mnist_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)

# Convert to numpy arrays and flatten
X = mnist_data.data.numpy()  # Shape: (60000, 28, 28)
X = X.reshape(len(X), -1)     # Flatten to (60000, 784)
y = mnist_data.targets.numpy()  # True labels (for evaluation)

# Normalize pixel values to [0, 1]
X = X / 255.0

# 2. Dimensionality Reduction with PCA
# -----------------------------------
# Reduce to 50 principal components (balances speed and info retention)
pca = PCA(n_components=50, random_state=42)
X_pca = pca.fit_transform(X)

print(f"Explained variance ratio: {sum(pca.explained_variance_ratio_):.2f}")

# 3. Apply K-Means Clustering
# --------------------------
kmeans = KMeans(n_clusters=10, random_state=42)
clusters = kmeans.fit_predict(X_pca)

# 4. Evaluation Metrics
# ---------------------
# Silhouette Score (higher is better, range [-1, 1])
silhouette = silhouette_score(X_pca, clusters)

# Adjusted Rand Index (compares with true labels, 1=perfect match)
ari = adjusted_rand_score(y, clusters)

print(f"\nClustering Results:")
print(f"- Silhouette Score: {silhouette:.3f}")
print(f"- Adjusted Rand Index: {ari:.3f}")

# 5. Visualize Cluster Centroids
# -----------------------------
# Get PCA-reconstructed centroids
centroids_pca = kmeans.cluster_centers_
centroids = pca.inverse_transform(centroids_pca)  # Back to original space

# Plot
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
for i, ax in enumerate(axes.flat):
    ax.imshow(centroids[i].reshape(28, 28), cmap='gray')
    ax.set_title(f"Cluster {i}")
    ax.axis('off')
plt.suptitle('Cluster Centroids (PCA-Reconstructed)', y=1.02)
plt.show()

# 6. Optional: t-SNE Visualization (for intuition)
# -----------------------------------------------
# (This part is slower but helpful for understanding)
from sklearn.manifold import TSNE

# Subsample for faster t-SNE
sample_idx = np.random.choice(len(X_pca), 1000, replace=False)
X_sample = X_pca[sample_idx]
y_sample = y[sample_idx]
c_sample = clusters[sample_idx]

# Run t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_sample)

# Plot
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=c_sample, cmap='tab10', alpha=0.6)
plt.colorbar(scatter, label='Cluster')
plt.title('t-SNE of Clusters (Subsampled)')
plt.show()

clusters = kmeans.fit_predict(X_pca)  # predicted cluster labels
y = mnist_data.targets.numpy()        # true digit labels

from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score
nmi = normalized_mutual_info_score(y, clusters)
print(f"- Normalized Mutual Information (NMI): {nmi:.3f}")

"""Customized k-means

# Initialize Model, Loss Function, and Optimizer
"""

class MyMLP ( nn . Module ) :
  def __init__ ( self , input_size , hidden_size , output_size ) :
    super ( MyMLP , self ) . __init__ ()
    self . net = nn . Sequential (
    nn . Linear ( input_size , hidden_size ) ,
    nn . ReLU () ,
    nn . Linear ( hidden_size , output_size )
    )

  def forward ( self , x ) :
    return self . net ( x )

input_size = 784 # 28 x28 images flattened
hidden_size = 128
output_size = 10 # Number of classes in MNIST
learning_rate = 0.001
batch_size = 64
epochs = 5

device = torch . device ("cuda" if torch . cuda . is_available () else "cpu")
model = MyMLP ( input_size , hidden_size , output_size ) . to ( device )

def count_parameters(model):
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total:,}")
    print(f"Trainable parameters: {trainable:,}")
    return total, trainable

count_parameters(model)

device = torch . device ("cuda" if torch . cuda . is_available () else "cpu")
model = MyMLP ( input_size , hidden_size , output_size ) . to ( device )
criterion = nn . CrossEntropyLoss ()
optimizer = optim . Adam ( model . parameters () , lr = learning_rate )

def count_parameters(model):
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total:,}")
    print(f"Trainable parameters: {trainable:,}")
    return total, trainable

# Instantiate and count
model = CNNLSTM().to(device)
count_parameters(model)

"""# Training Loop


"""

for epoch in range ( epochs ) :
    model . train ()
    for batch_idx , ( data , targets ) in enumerate ( train_loader ) :
        data = data . view ( data . size (0) , -1) . to ( device )
        targets = targets . to ( device )
        scores = model ( data )
        loss = criterion ( scores , targets )
        optimizer . zero_grad ()
        loss . backward ()
        optimizer . step ()
    print ( f" Epoch [{ epoch +1}/{ epochs }] , Loss : { loss . item () :.4f}")

"""# Evaluation"""

model . eval ()
correct = 0
total = 0
with torch . no_grad () :
   for data , targets in test_loader :
        data = data . view ( data . size (0) , -1) . to ( device )
        targets = targets . to ( device )
        outputs = model ( data )
        _ , predicted = torch .max ( outputs . data , 1)
        total += targets . size (0)
        correct += ( predicted == targets ) .sum () . item ()

print ( f" Test Accuracy : {100 * correct / total :.2f}%")

import torch
import torch.nn.functional as F
from sklearn.metrics import adjusted_rand_score

# 1. Extract features from the hidden layer
def extract_hidden_features(model, loader):
    model.eval()
    features = []
    labels = []
    with torch.no_grad():
        for data, target in loader:
            data = data.view(data.size(0), -1).to(device)
            out = model.net[0](data)  # First Linear layer output
            out = F.relu(out)         # Apply ReLU (same as model.net[1])
            features.append(out)
            labels.append(target)
    return torch.cat(features), torch.cat(labels)

# 2. Initialize k random centroids
def init_centroids(X, k):
    indices = torch.randperm(X.size(0))[:k]
    return X[indices]

# 3. Cosine similarity
def cosine_similarity(X, centroids):
    X_norm = X / X.norm(dim=1, keepdim=True)
    C_norm = centroids / centroids.norm(dim=1, keepdim=True)
    return torch.mm(X_norm, C_norm.T)

# 4. Assign clusters using cosine similarity
def assign_clusters_cosine(X, centroids):
    sims = cosine_similarity(X, centroids)
    return torch.argmax(sims, dim=1)

# 5. Update centroids
def update_centroids(X, cluster_assignments, k):
    new_centroids = []
    for i in range(k):
        cluster_points = X[cluster_assignments == i]
        if len(cluster_points) == 0:
            new_centroids.append(X[torch.randint(0, X.size(0), (1,))])
        else:
            new_centroids.append(cluster_points.mean(dim=0))
    return torch.stack(new_centroids)

# 6. Full K-Means loop with cosine
def custom_kmeans(X, k=10, max_iters=100, tol=1e-4):
    centroids = init_centroids(X, k)
    for i in range(max_iters):
        cluster_assignments = assign_clusters_cosine(X, centroids)
        new_centroids = update_centroids(X, cluster_assignments, k)
        if torch.norm(centroids - new_centroids) < tol:
            break
        centroids = new_centroids
    return cluster_assignments, centroids

# === Run Everything ===

# Extract hidden layer features
X, y_true = extract_hidden_features(model, test_loader)
X = X.to(device)

# Cluster using custom cosine-similarity KMeans
k = 10
cluster_labels, centroids = custom_kmeans(X, k=k)

# Show sample cluster assignments
print("Sample cluster assignments:", cluster_labels[:20].tolist())

# Evaluate clustering quality (optional)
print(f"Adjusted Rand Index vs true labels: {adjusted_rand_score(y_true.cpu(), cluster_labels.cpu()):.4f}")

nmi = normalized_mutual_info_score(y_true.cpu(), cluster_labels.cpu())
print(f"- Normalized Mutual Information (NMI): {nmi:.3f}")

import matplotlib.pyplot as plt
import numpy as np

# Your obtained NMI scores
kmeans_nmi = 0.1935
som_nmi = 0.0976
modified_kmeans_nmi = 0.721
kmeans_pca = 0.490

# Data for plotting
methods = ['K-means(CNN-LSTM)', 'SOM(CNN-LSTM)', 'Modified K-means(MLP)', 'K-means(PCA)']
nmi_scores = [kmeans_nmi, som_nmi, modified_kmeans_nmi, kmeans_pca]
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#9467bd']  # Blue, Orange, Green, Purple

# Create figure
plt.figure(figsize=(10, 6), dpi=100)  # Wider figure for 4 bars

# Create bars
bars = plt.bar(methods, nmi_scores, color=colors, width=0.6)

# Add value labels on top of each bar
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.4f}',
             ha='center', va='bottom',
             fontsize=12, fontweight='bold')

# Add horizontal line at random baseline (0) and perfect score (1)
plt.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.7)
plt.axhline(y=1, color='green', linestyle=':', linewidth=1, alpha=0.5)

# Chart formatting
plt.title('Clustering Performance Comparison\n(Normalized Mutual Information)',
          fontsize=14, pad=20)
plt.ylabel

