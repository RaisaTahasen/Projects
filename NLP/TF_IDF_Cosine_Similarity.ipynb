{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0PcSVGcT/fWPFc6OfNV/x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaisaTahasen/Project/blob/main/NLP/TF_IDF_Cosine_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1"
      ],
      "metadata": {
        "id": "dSxJ1c9HwVKH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vznM4vXIfiQ",
        "outputId": "298ca97d-fe5b-4bdd-9beb-4f57f345f442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('nps_chat')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdPSB0KwK1qp",
        "outputId": "4e4c766e-bc16-436d-c095-b660f6654124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import nps_chat"
      ],
      "metadata": {
        "id": "xCJuTap1K7q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download('all')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRi3AW_bL62q",
        "outputId": "9d96c3e5-5f8a-4199-8d9a-8e456e6f4451",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *\n"
      ],
      "metadata": {
        "id": "BbTn8R9NKYqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1a"
      ],
      "metadata": {
        "id": "vLm-t91SyP_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(set(text1))\n",
        "print(f\"Number of unique words in Moby Dick {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph-s7D7GMhpJ",
        "outputId": "aee4117a-e50c-4ed8-dab6-027c44330df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in Moby Dick 19317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1b"
      ],
      "metadata": {
        "id": "fTrpJDAwyCbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = len(text1)\n",
        "print(f\"Total number of words in Moby Dick {length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atwMgErLM1Wo",
        "outputId": "5c7b51b7-7ef8-41c8-abd8-094b0230aa8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in Moby Dick 260819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1c"
      ],
      "metadata": {
        "id": "Cn-_wqC2x91K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_words ={}\n",
        "for w in text1:\n",
        "  if w in num_of_words:\n",
        "     num_of_words[w]+=1\n",
        "  else:\n",
        "    num_of_words[w] = 1\n",
        "\n",
        "print(\"Sample word counts (first 10):\")\n",
        "for i, (word, count) in enumerate(num_of_words.items()):\n",
        "    print(f\"{word} : {count}\")\n",
        "    if i >= 9:\n",
        "        break\n",
        "\n",
        "print(f\"\\nTotal unique words (including punctuation): {len(num_of_words)}\")"
      ],
      "metadata": {
        "id": "FEv8MBZvSEM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97573085-0475-45eb-e58e-34347c192684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample word counts (first 10):\n",
            "[ : 3\n",
            "Moby : 84\n",
            "Dick : 84\n",
            "by : 1137\n",
            "Herman : 1\n",
            "Melville : 1\n",
            "1851 : 3\n",
            "] : 1\n",
            "ETYMOLOGY : 1\n",
            ". : 6862\n",
            "\n",
            "Total unique words (including punctuation): 19317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1d"
      ],
      "metadata": {
        "id": "sf4Atpr6x0GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sort = dict(sorted(num_of_words.items(),key=lambda item: item[1], reverse=True))\n",
        "print(\"Top 10 most frequent words:\")\n",
        "for i, (word, count) in enumerate(sort.items()):\n",
        "    print(f\"{word} : {count}\")\n",
        "    if i >= 9:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoGX_TuOkt-C",
        "outputId": "ea1767fa-0928-4168-c3cb-ad835b11c97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most frequent words:\n",
            ", : 18713\n",
            "the : 13721\n",
            ". : 6862\n",
            "of : 6536\n",
            "and : 6024\n",
            "a : 4569\n",
            "to : 4542\n",
            "; : 4072\n",
            "in : 3916\n",
            "that : 2982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1e"
      ],
      "metadata": {
        "id": "GTWiYNT7xyit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "print(\"Top 10 frequent words except punctuation\")\n",
        "n=0\n",
        "for i, (word, count) in enumerate(sort.items()):\n",
        "    if word in string.punctuation:\n",
        "      pass\n",
        "    else:\n",
        "      print(f\"{word} : {count}\")\n",
        "      n+=1\n",
        "    if n > 9:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjglR9qYm7NB",
        "outputId": "71754361-40d3-4635-aac3-963ed58a5a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 frequent words except punctuation\n",
            "the : 13721\n",
            "of : 6536\n",
            "and : 6024\n",
            "a : 4569\n",
            "to : 4542\n",
            "in : 3916\n",
            "that : 2982\n",
            "his : 2459\n",
            "it : 2209\n",
            "I : 2124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 2a"
      ],
      "metadata": {
        "id": "IusoEEwO8yJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GpfR4KT8xTL",
        "outputId": "1e6e9114-cc60-4e5f-ca74-fa5c23ea30c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/IMDB Dataset.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24_pm2o5-bh7",
        "outputId": "a0f639a2-a376-46e1-eb45-4dfe3c850843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_reviews = len(df)\n",
        "print(f\"Number of reviews {num_of_reviews}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oebbpJKyBPA1",
        "outputId": "5855912a-b0aa-4e60-bc31-c58f4670571a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reviews 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTshaD7qCysz",
        "outputId": "9df0209e-7769-44ae-f346-366cda396cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count words\n",
        "def count_num_of_words(text):\n",
        "    return len(word_tokenize(text))\n",
        "\n",
        "# Function to count sentences\n",
        "def count_num_of_sentences(text):\n",
        "    return len(sent_tokenize(text))\n",
        "\n",
        "# Add word and sentence counts as new columns in the DataFrame\n",
        "df['WordCount'] = df['review'].apply(count_num_of_words)\n",
        "df['SentenceCount'] = df['review'].apply(count_num_of_sentences)\n",
        "\n",
        "\n",
        "word_index = df['WordCount'].idxmax()\n",
        "longest_word_review = df.loc[word_index]\n",
        "\n",
        "sen_index = df['SentenceCount'].idxmax()\n",
        "longest_sentence_review = df.loc[sen_index]"
      ],
      "metadata": {
        "id": "2FEtPrKcC10a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Longest review in terms of words:\")\n",
        "print(f\"Review: {longest_word_review['review']}\")\n",
        "print(f\"Word Count: {longest_word_review['WordCount']}\")\n",
        "print(f\"Row Number: {word_index}\")\n",
        "print(\"\\nLongest review in terms of sentences:\")\n",
        "print(f\"Review: {longest_sentence_review['review']}\")\n",
        "print(f\"Sentence Count: {longest_sentence_review['SentenceCount']}\")\n",
        "print(f\"Row Number: {sen_index}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFPLLEWjC4TK",
        "outputId": "bc83bff4-ed29-4c36-b0bd-186f1fd5dcc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest review in terms of words:\n",
            "Review: There's a sign on The Lost Highway that says:<br /><br />*MAJOR SPOILERS AHEAD*<br /><br />(but you already knew that, didn't you?)<br /><br />Since there's a great deal of people that apparently did not get the point of this movie, I'd like to contribute my interpretation of why the plot makes perfect sense. As others have pointed out, one single viewing of this movie is not sufficient. If you have the DVD of MD, you can \"cheat\" by looking at David Lynch's \"Top 10 Hints to Unlocking MD\" (but only upon second or third viewing, please.) ;)<br /><br />First of all, Mulholland Drive is downright brilliant. A masterpiece. This is the kind of movie that refuse to leave your head. Not often are the comments on the DVDs very accurate, but Vogue's \"It gets inside your head and stays there\" really hit the mark.<br /><br />David Lynch deserves praise for creating a movie that not only has a beautifully stylish look to it - cinematography-wise, has great acting (esp. Naomi Watts), a haunting soundtrack by Badalamenti, and a very dream-like quality to it -- but on top of it all it also manages to involve the viewer in such a way that few movies have before. (After all, when is the last time you saw a movie that just wouldn't leave your mind and that everyone felt compelled to talk and write about, regardless of whether they liked it or hated it?)<br /><br />Allright, enough about all that, it's time to justify those statements.<br /><br />Most people that have gone through some effort to try to piece the plot together will have come to the conclusion that the first half of the picture is an illusion/a dream sequence.<br /><br />Of course, that's too bad for all those trying to make sense of the movie by expecting \"traditional\" methods in which the story is laid out in a timely, logic and linear manner for the viewer. But for those expecting that, I urge you to check the name of the director and come back again. ;)<br /><br />MD is the story of the sad demise of Diane Selwyn, a wannabe-actor who is hopelessly in love with another actor, Camilla Rowles. Due to Diane's lack of talent, she is constantly struggling to advance her career, and feels she failed to deliver on her own and her parents' expectations. Upon realizing that Camilla will never be hers (C. becomes engaged with Adam Kesher, the director), she hires a hitman to get rid of her, and subsequently has to deal with the guilt that it produces.<br /><br />The movie first starts off with what may seem as a strange opening for this kind of thriller; which is some 50s dance/jitterbug contest, in which we can see the main character Betty giving a great performance. We also see an elderly couple (which we will see twice more throughout the movie) together with her, and applauding her.<br /><br />No, wait. This is what most people see the first time they view it. There's actually another very significant fact that is given before the credits - the camera moving into an object (although blurry) and the scene quickly fading out. If you look closely, the object is actually a pillow, revealing that what follows is a dream.<br /><br />The main characters seen in the first half of the movie:<br /><br />Betty: Diane Selwyn's imaginary self, used in the first half of the movie that constitutes the \"dream-sequence\" - a positive portrayal of a successful, aspiring young actor (the complete opposite of Diane). 'Betty' was chosen as the name as that is the real name of the waitress at Winkies. Notice that in the dream version, the waitresses' name is 'Diane'.<br /><br />Rita: The fantasy version of Camilla Rhodes that, through Diane's dream, and with the help of an imaginary car-accident, is turned into an amnesiac. This makes her vulnerable and dependent on Diane's love. She is then conveniently placed in Betty/Diane's aunt's luxurious home which Betty has been allowed to stay in.<br /><br />Coco: In real life, Adam's mother. In the dream part, the woman in charge of the apartment complex that Betty stays in. She's mainly a strong authority figure, as can be witnessed in both parts of the film.<br /><br />Adam: The director. We know from the second half that he gets engaged with Camilla. His sole purpose for being in the first half of the movie is only to serve as a punching bag for Betty/Diane, since she develops such hatred towards him.<br /><br />Aunt Ruth: Diane's real aunt, but instead of being out of town, she is actually dead. Diane inherited the money left by her aunt and used that to pay for Camilla's murder.<br /><br />Mr. Roach: A typical Lynchian character. Not real; appears only in Diane's dream sequence. He's a mysterious, influential person that controls the chain of events in the dream from his wheelchair. He serves much of the same function as the backwards-talking dwarf (which he also plays) in Twin Peaks.<br /><br />The hitman: The person that murders Camilla. This character is basically the same in both parts of the movie, although rendered in a slightly more goofy fashion in the dream sequence (more on that below).<br /><br />Now, having established the various versions of the characters in the movie, we can begin to delve into the plot. Of course I will not go into every little detail (neither will I lay it out chronologically), but I will try to explain some of the important scenes, in relation to Lynch' \"hint-sheet\".<br /><br />As I mentioned above, Camilla was re-produced as an amnesiac through her improbable survival of a car-accident in the first 10 minutes of the movie, which left her completely vulnerable. What I found very intriguing with MD, is that Lynch constantly gives hints on what is real and what isn't. I've already mentioned the camera moving into the pillow, but notice how there's two cars riding in each lane approaching the limo.<br /><br />Only one of the cars actually hit the limo; what about the other? Even if they stayed clear of the accident themselves, wouldn't they try to help the others, or at least call for help? My theory is that, since this is a dream, the presence of the other car is just set aside, and forgotten about. Since, as Rogert Ebert so eloquently puts it \"Like real dreams, it does not explain, does not complete its sequences, lingers over what it finds fascinating, dismisses unpromising plotlines.\"<br /><br />Shortly after Rita crawls down from the crash site at Mulholland Dr., and makes her way down the hillside and sneaks into Aunt Ruth's apartment, Betty arrives and we see this creepy old couple driving away, staring ghoulishly at each other and grinning at themselves and the camera. This is the first indication that what we're seeing is a nightmare.<br /><br />Although the old couple seem to be unfamiliar to Betty, I think they're actually her parents (since they were applauding her at the jitterbug contest). Perhaps she didn't know them all that well, and didn't really have as good a relationship with them as she wanted, so the couple is shown as very pleasant and helpful to her in the dream. They also represent her feelings of guilt from the murder, and Diane's sense of unfulfillment regarding her unachieved goals in her life.<br /><br />A rather long and hilarious scene is the one involving the hitman. Diane apparently sees him as the major force behind the campaign trying to pressure the director to accept Camilla's part in the movie (from Adam's party in the second half of the movie), and he therefore occupies a major part of her dream. Because of her feelings of guilt and remorse towards the murder of Camilla, a part of her wants him to miss, so she turns him into a dumb criminal.<br /><br />This scene, I think, is also Lynch's attempt at totally screwing his audience over, since they're given a false pretence in which to view the movie.<br /><br />Gotta love that 'Something just bit me bad' line, though. :)<br /><br />The next interesting scene is the one with the two persons at Twinkies, who are having a conversation about how one of them keep having this recurring nightmare involving a man which is seen by him through a wall outside of the diner that they're sitting in. After a little talk, they head outside and keep walking toward the corner of a fence, accompanied of course by excellent music matching the mood of the scene.<br /><br />When reaching the corner, a bum-like character with a disfigured face appears out from behind the corner, scaring the living crap out of the man having the nightmare. This nightmare exists only in Diane's mind; she saw that guy in the diner when paying for the murder. So, in short, her obessions translate into that poor guy's nightmares. The bum also signifies Diane's evil side, as can be witnessed later in the movie.<br /><br />The Cowboy constitutes (along with the dwarf) one of the strange characters that are always present in the Lynchian landscape -- Diane only saw him for a short while at Adam's party, but just like our own dreams can award insignificant persons that we hardly know a major part in our dreams, so can he be awarded an important part in her dream. We are also given further clues during his scenes that what we're seeing is not real (his sudden disappearance, etc.)<br /><br />The Cowboy is also used as a tool to mock the Director, when he meets up with him at the odd location (the lights here give a clear indication that this is part of a dream). Also notice how he says that he will appear one more time if he (Adam) does good, or two more times if he does bad. Throughout the movie he appears two more times, indicating to Diane that she did bad. He is also the one to wake her up to reality (that scene is probably an illusion made to fit into her requirements of him appearing twice), and shortly thereafter she commits suicide.<br /><br />The espresso-scene with the Castigliane brothers (where we can see Badalamenti, the composer, as Luigi) is probably a result of the fact that Diane was having an espresso just before Camilla and Adam made their announcement at Adam's party in the second half. It could at the same time also be a statement from Lynch.<br /><br />During the scene in which they enter Diane's apartment, the body lying in the bed is Camilla, but notice how she's assumed Diane's sleeping position; Diane is seeing herself in her own dream, but the face is not hers, although it had the same wounds on the face as Diane would have after shooting herself. This scene is also filled with some genuine Lynchian creepiness. Since Diane did not know where (or when) the hitman would get to Camilla and finish her off, she just put her into her own home.<br /><br />In real life, Diane's audition for the movie part was bad. In her dream, she delivers a perfect audition - leaving the whole crew ecstatic about her performance.<br /><br />Also interesting is the fact that the money that in real-life was used to pay for Camilla's murder now appears in Rita/Camilla's purse. This is part of Diane's undoing of her terrible act by effectively being given the money back, as the murder now hasn't taken place.<br /><br />When her neighbor arrives to get her piano-shaped ashtray, another hint is given; she takes the ashtray from her table and leaves, yet later when Camilla and Betty have their encounter on the couch, we see the ashtray appear again when the camera pans over the table, suggesting that Betty's encounter with the neighbor was a fantasy.<br /><br />The catch phrase of the movie Adam is auditioning actresses for is \"She is the girl\"; which are the exact same words that Diane uses when giving the hitman Camilla's photo resume.<br /><br />The blue box and the key represent the major turning point in the movie, and is where the true identities of the characters are revealed. There's much symbolism going on here; the box may represent Diane's future (it's empty), or it may be a sort of a Pandora's box (the hitman laughs when she asks him what the key will open). Either way, it is connected to the murder by means of the blue key (which is placed next to her after the murder has taken place). The box is also seen at the end of the movie in the hands of the disfigured bum.<br /><br />Club Silencio is a neat little addition to further remind the viewer that what s/he is viewing is not real. It also signifies that Diane is about to wake up to her reality (her reality being a nightmare that she is unable to escape from, even in her dreams).<br /><br />During the chilling scene at the end where the creepy old couple reappear, Diane is tormented in such a way that she sees suicide as the only way out in order to escape the screams and to avoid being haunted by her fears.<br /><br />Anyway, that is my $0.02. Hope this could help people from bashing out at this movie and calling it 'the worst movie ever' or something to that effect, without realizing the plot.<br /><br />As usual, Lynch is all about creating irrational fears, and he certainly achieves that with this picture as well.<br /><br />10 out of 10.\n",
            "Word Count: 2911\n",
            "Row Number: 40521\n",
            "\n",
            "Longest review in terms of sentences:\n",
            "Review: Smallville episode Justice is the best episode of Smallville ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! It's my favorite episode of Smallville! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "Sentence Count: 282\n",
            "Row Number: 39182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2b"
      ],
      "metadata": {
        "id": "5by1l-7XxiLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "def clean_review(txt):\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    punctuation = string.punctuation\n",
        "\n",
        "    words = word_tokenize(txt)\n",
        "\n",
        "    cl_words = []\n",
        "    for word in words:\n",
        "       if word not in stop_words and word not in punctuation:\n",
        "         cl_words.append(word)\n",
        "\n",
        "    return ' '.join(cl_words)  # Join words back into a cleaned string\n",
        "\n",
        "# Apply the cleaning function to the review column\n",
        "df['CleanedReview'] = df['review'].apply(clean_review)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(df[['review', 'CleanedReview']].head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUai8FXPK13U",
        "outputId": "0910a593-fb69-401d-d139-48797e2d7c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review  \\\n",
            "0  One of the other reviewers has mentioned that ...   \n",
            "1  A wonderful little production. <br /><br />The...   \n",
            "2  I thought this was a wonderful way to spend ti...   \n",
            "\n",
            "                                       CleanedReview  \n",
            "0  One reviewers mentioned watching 1 Oz episode ...  \n",
            "1  A wonderful little production br br The filmin...  \n",
            "2  I thought wonderful way spend time hot summer ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2c"
      ],
      "metadata": {
        "id": "MENHlfUjxo_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['CleanedReview'] = df['CleanedReview'].str.lower()\n",
        "\n",
        "with open('cleaned_reviews.txt', 'w') as file:\n",
        "    for r in df['CleanedReview']:\n",
        "        file.write(r + '\\n')\n",
        "\n",
        "with open('cleaned_reviews.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    print(f\"Total reviews saved: {len(lines)}\")\n",
        "    print(f\"Total reviews in DataFrame: {len(df)}\")\n",
        "\n",
        "print(df['CleanedReview'].head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQXmjx8-ZCcJ",
        "outputId": "88707346-462c-400a-ee3e-1c4f350aa2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reviews saved: 50000\n",
            "Total reviews in DataFrame: 50000\n",
            "0    one reviewers mentioned watching 1 oz episode ...\n",
            "1    a wonderful little production br br the filmin...\n",
            "2    i thought wonderful way spend time hot summer ...\n",
            "Name: CleanedReview, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3a"
      ],
      "metadata": {
        "id": "CDNJD9DjwsT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['CleanedReview'])\n",
        "\n",
        "\n",
        "word_counts = (tfidf_matrix > 0).sum(axis=1).A1\n",
        "print(f\"\\nTF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
        "print(f\"\\nVocabulary Size: {tfidf_matrix.shape[1]}\")\n",
        "\n",
        "print(\"\\nSparse TF-IDF Matrix (First 5 Rows, 10 Features):\")\n",
        "print(tfidf_matrix[:5, :10].toarray())  # First 5 rows, first 10 columns\n",
        "\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()[:10]\n",
        "print(\"\\nFeature Names for First 10 Features:\")\n",
        "print(feature_names)\n",
        "\n",
        "\n",
        "# max_word_count = word_counts.max()\n",
        "\n",
        "# max_word_count_index = word_counts.argmax()\n",
        "\n",
        "# print(\"\\nReview with highest word count:\")\n",
        "# print(f\"Word Count {max_word_count}\")\n",
        "# print(f\"Row Count {max_word_count_index}\")\n"
      ],
      "metadata": {
        "id": "63otLepBpwwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1145ab-09be-4ef1-dbfd-a3c9a23ddc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix Shape: (50000, 101880)\n",
            "\n",
            "Vocabulary Size: 101880\n",
            "\n",
            "Sparse TF-IDF Matrix (First 5 Rows, 10 Features):\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "Feature Names for First 10 Features:\n",
            "['00' '000' '00000000000' '0000000000001' '00000001' '00001' '00015'\n",
            " '000dm' '000s' '001']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names from the TF-IDF vectorizer\n",
        "tfidf_vectorizer.get_feature_names_out()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXwmrvQdlgke",
        "outputId": "a63dc42b-3863-40f3-840c-8ff1775abdce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['00', '000', '00000000000', ..., 'mijewski', '', ''],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3b"
      ],
      "metadata": {
        "id": "ZlQtwSJpxYFj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH8zazeyUfXH",
        "outputId": "b19b7b83-8cf2-4cb3-bf87-4ef2ccf1e3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarities:\n",
            "man and woman : 0.8323\n",
            "cat and dog : 0.8798\n",
            "king and queen : 0.7508\n",
            "\n",
            "King - Man + Woman = Queen: 0.7834\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "file_path = \"/content/drive/MyDrive/glove.6B.100d.txt\"\n",
        "\n",
        "def load_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as c:\n",
        "        for line in c:\n",
        "            val = line.split()\n",
        "            word = val[0]\n",
        "            vector = np.array(val[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Cosine similarity\n",
        "def cos_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
        "\n",
        "\n",
        "glove_file = file_path\n",
        "\n",
        "embeddings = load_embeddings(glove_file)\n",
        "\n",
        "\n",
        "words = ['man', 'woman', 'cat', 'dog', 'king', 'queen']\n",
        "vectors = {word: embeddings[word] for word in words}\n",
        "\n",
        "# Compute cosine similarities\n",
        "similarities = {\n",
        "    (\"man and woman\"): cos_similarity(vectors[\"man\"], vectors[\"woman\"]),\n",
        "    (\"cat and dog\"): cos_similarity(vectors[\"cat\"], vectors[\"dog\"]),\n",
        "    (\"king and queen\"): cos_similarity(vectors[\"king\"], vectors[\"queen\"]),\n",
        "}\n",
        "\n",
        "\n",
        "king_man_woman = vectors[\"king\"] - vectors[\"man\"] + vectors[\"woman\"]\n",
        "\n",
        "\n",
        "queen_similarity = cos_similarity(king_man_woman, vectors[\"queen\"])\n",
        "\n",
        "print(\"Cosine Similarities:\")\n",
        "for pair, sim in similarities.items():\n",
        "    print(f\"{pair} : {sim:.4f}\")\n",
        "\n",
        "print(f\"\\nKing - Man + Woman = Queen: {queen_similarity:.4f}\")\n"
      ]
    }
  ]
}